
from crag import run_corrective_rag
# streamlit run app.py
import streamlit as st
import re
from qa_backend import run_qa, load_documents_from_upload
available_models = [
    "deepseek-r1:8b",
    "qwen3:14b",
    "qwen2.5:7b",
    "gemma3:27b",
    "deepseek-r1:32b"
]

st.title("ğŸ¤– AI_RAGå•ç­”æ©Ÿå™¨äºº")

# é è¨­åƒæ•¸ï¼ˆå¯å„²å­˜ï¼‰
if "model_settings" not in st.session_state:
    st.session_state.model_settings = {
        "llm_model": "deepseek-r1:8b",
        "temperature": 0.7,
        "top_p": 0.9,
    }

# æŒ‰éˆ•é»æ“Šå¾Œçš„å°è¦–çª—ï¼ˆpopoverï¼‰
with st.popover("âš™ï¸ æ¨¡å‹åƒæ•¸è¨­å®š", use_container_width=True):
    llm_model = st.selectbox(
        "é¸æ“‡æ¨¡å‹",
        options=available_models,
        index=available_models.index(st.session_state.model_settings["llm_model"]) if st.session_state.model_settings["llm_model"] in available_models else 0
    )
    temperature = st.slider("æº«åº¦ (temperature)", 0.0, 1.0, st.session_state.model_settings["temperature"])
    top_p = st.slider("Top-p", 0.0, 1.0, st.session_state.model_settings["top_p"])

    if st.button("âœ… å„²å­˜åƒæ•¸", key="save_model_settings"):
        st.session_state.model_settings = {
            "llm_model": llm_model,
            "temperature": temperature,
            "top_p": top_p,
        }
        st.success("å·²å„²å­˜åƒæ•¸ï¼")

uploaded_files = st.file_uploader("**å¯ä¸Šå‚³æ–‡å­—æª”(txt, pdf) æˆ–åœ–ç‰‡æª”(jpg, jpeg, png)**", type=["txt", "pdf", "jpg", "png"], accept_multiple_files=True)      # é€™è¡Œæœ‰æ”¹
# ollama_url = "http://172.20.5.116:11434"
ollama_url = "http://127.0.0.1:11434"    # Pohan's lab server

query = st.text_area("è«‹è¼¸å…¥ä½ çš„å•é¡Œ", value="è¯é›„çš„çµ•å­¸æ˜¯ä»€éº¼ï¼Ÿä»–çš„æ­»æ³•ç‚ºä½•ï¼Ÿ")

# let thinking progress become collapsible(å¯æ”¶æ”¾çš„)
def extract_answer_and_thought(text: str):
    # æ‹†å‡º <think> å’Œå›ç­”
    match = re.search(r"<think>([\s\S]*?)</think>", text)
    if match:
        thought = match.group(1).strip()
        answer = re.sub(r"<think>[\s\S]*?</think>", "", text).strip()
    else:
        answer = text.strip()
        thought = ""
    return answer, thought


if st.button("å›ç­”å•é¡Œ"):
    if uploaded_files and query:
        with st.spinner("è™•ç†ä¸­..."):
            try:
                all_docs = []
                for f in uploaded_files:
                    all_docs.extend(load_documents_from_upload(f, ollama_url))
                result = run_corrective_rag(
                    docs=all_docs,
                    query=query,
                    ollama_url=ollama_url,
                    llm_model=st.session_state.model_settings["llm_model"],
                    embedding_model="bge-m3",
                    temperature=st.session_state.model_settings["temperature"],
                    top_p=st.session_state.model_settings["top_p"],
                    k=5
                )

                # å·¦å³åˆ†æ¬„
                left_col, right_col = st.columns([2, 1])

                with left_col:
                    st.markdown("### ğŸ“˜ å›ç­”ï¼š")
                    answer, thought = extract_answer_and_thought(result["answer"])
                    st.markdown(answer)

                    if thought:
                        with st.expander("ğŸ’­ æ€è€ƒéç¨‹"):
                            st.markdown(thought)

                with right_col:
                    st.markdown("### ğŸ” åŒ¹é…æ®µè½ï¼š")
                    for i, chunk in enumerate(result["chunks"]):
                        with st.expander(f"Chunk #{i+1}ï¼ˆåˆ†æ•¸: {chunk['score']:.4f}ï¼‰"):
                            st.write(chunk["content"])

            except Exception as e:
                st.error(f"éŒ¯èª¤ï¼š{e}")
    else:
        st.warning("è«‹å…ˆä¸Šå‚³æª”æ¡ˆä¸¦è¼¸å…¥å•é¡Œã€‚")

# è¼‰å…¥æª”æ¡ˆèˆ‡åƒæ•¸å¾Œ


# é¡¯ç¤ºåˆç¨¿èˆ‡æœ€çµ‚æ ¡æ­£å›ç­”
st.markdown("### ğŸ’­ åˆç¨¿å›ç­”ï¼š")
st.markdown(result["draft"])
st.markdown("### ğŸ“˜ æœ€çµ‚æ ¡æ­£å›ç­”ï¼š")
st.markdown(result["answer"])